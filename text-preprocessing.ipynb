{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8829f406",
   "metadata": {},
   "source": [
    "#  NLP Pipeline\n",
    "## Text Preprocessing\n",
    "\n",
    "1. Data Acquisition\n",
    "2. Text Processing\n",
    "    * Text cleanup\n",
    "    * Basic Preprocessing\n",
    "    * Advance Preprocessing\n",
    "3. Feature Engineering\n",
    "4. Modeling\n",
    "5. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f20c49",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aadf83e",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "    * CSB file (table)\n",
    "    * Database -- Data Agensis\n",
    "    * Less Data -- Data Argumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d5fde",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "1. Text cleanup\n",
    "2. Basic Preprocessing\n",
    "3. Advance Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5991d4",
   "metadata": {},
   "source": [
    "# Text cleanup\n",
    "\n",
    "1. html tag\n",
    "2. emoji\n",
    "3. spelling check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abaa5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a5c9be",
   "metadata": {},
   "source": [
    "**pandas is an open-source data manipulation and analysis library for Python. It provides data structures and functions that make working with structured data, such as CSV files, Excel spreadsheets, SQL databases, and more, much easier and efficient. It's widely used in data science, data analysis, and data manipulation tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080f733",
   "metadata": {},
   "source": [
    "## Here are some key components and concepts of the pandas library:\n",
    "\n",
    "1. Data Structures:\n",
    "\n",
    "  * Series: A one-dimensional labeled array, similar to a column in a spreadsheet or a list in Python.\n",
    "  * DataFrame: A two-dimensional labeled data structure with columns that can be of different types (numeric, string, boolean, etc.). It's similar to a table in a database or a spreadsheet.\n",
    "  \n",
    "2. Key Features:\n",
    "\n",
    " * Data Reading and Writing: pandas can read data from various file formats like CSV, Excel, SQL databases, and more. It also supports writing data to these formats.\n",
    "  * Indexing and Selection: pandas provides flexible ways to index and select data, including label-based indexing, integer-based indexing, and boolean indexing.\n",
    "   * Data Cleaning and Transformation: pandas allows you to handle missing data, duplicate data, and perform operations like filtering, sorting, grouping, and reshaping.\n",
    "   * Aggregation and Statistics: You can easily compute summary statistics, apply functions to groups of data, and perform aggregations.\n",
    "  * Merging and Joining: pandas supports combining data from different sources through operations like merging and joining.\n",
    "   * Time Series Data: pandas has robust support for handling time series data, including date and time parsing, resampling, and time-based calculations.\n",
    "  * Visualization: While pandas itself doesn't provide advanced visualization capabilities, it can integrate well with libraries like Matplotlib and Seaborn for data visualization.\n",
    "3. Getting Started:\n",
    "To use pandas, you need to import the library. Common import conventions are:  \n",
    " **import pandas as pd**\n",
    " \n",
    "4. Basic Usage:\n",
    "\n",
    "  * Creating a DataFrame from data: df = pd.DataFrame(data)\n",
    "  * Reading data from a CSV file: df = pd.read_csv('data.csv')\n",
    "  * Displaying the first few rows: df.head()\n",
    "  * Indexing and selecting data: df['column_name'] or df.loc[row_label]\n",
    "  * Applying functions: df['column_name'].apply(func)\n",
    "  * Grouping data: df.groupby('group_column').agg(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23d403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed0b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765a1150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e89438e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a243803f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ebf900",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1ce63",
   "metadata": {},
   "source": [
    "# Html Tag Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3615f",
   "metadata": {},
   "source": [
    "# Importing the re Module:\n",
    "**The code begins by importing the re module, which stands for \"regular expressions.\" This module provides functions for working with regular expressions, which are powerful tools for pattern matching and manipulation of strings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a21e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed2efd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3758398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Movie 1 Actor - Aamir Khan Click here to download'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html_tags(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d0db280",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f87e19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times in the last 25 years. paul lukas\\' performance brings tears to my eyes, and bette davis, in one of her very few truly sympathetic roles, is a delight. the kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch. and the mother\\'s slow awakening to what\\'s happening in the world and under her own roof is believable and startling. if i had a dozen thumbs, they\\'d all be \"up\" for this movie.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aa254c",
   "metadata": {},
   "source": [
    "# Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d398eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "650807d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text2 = 'Check out my notebook http://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text3 = 'Google search here www.google.com'\n",
    "text4 = 'For notebook click https://www.kaggle.com/campusx/notebook8223fc1abb to search check www.google.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bb5eaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For notebook click  to search check '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ebcf9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out my notebook '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f12a8",
   "metadata": {},
   "source": [
    "# Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a462f107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string,time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f71de88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f59be00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0420f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'string. With. Punctuation?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6640e011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string With Punctuation\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(remove_punc(text))\n",
    "time1 = time.time() - start\n",
    "print(time1*50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1c209f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b0ced76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "remove_punc1(text)\n",
    "time2 = time.time() - start\n",
    "print(time2*50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62871cd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m time1\u001b[38;5;241m/\u001b[39mtime2\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "time1/time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a64e840e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times in the last 25 years. paul lukas\\' performance brings tears to my eyes, and bette davis, in one of her very few truly sympathetic roles, is a delight. the kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch. and the mother\\'s slow awakening to what\\'s happening in the world and under her own roof is believable and startling. if i had a dozen thumbs, they\\'d all be \"up\" for this movie.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d7b61ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punc1(df['review'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69210002",
   "metadata": {},
   "source": [
    "# Chat Words Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "194fc678",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "    'LOL': 'laugh out loud',\n",
    "    'BRB': 'be right back',\n",
    "    'IMHO': 'in my humble opinion',\n",
    "    'BTW': 'by the way',\n",
    "    'ROFL': 'rolling on the floor laughing',\n",
    "    'TTYL': 'talk to you later',\n",
    "    'FYI': 'for your information',\n",
    "    'OMG': 'oh my god',\n",
    "    'IDK': \"I don't know\",\n",
    "    'BFF': 'best friends forever',\n",
    "    'IMO': 'in my opinion',\n",
    "    'TMI': 'too much information',\n",
    "    'JK': 'just kidding',\n",
    "    'GTG': 'got to go',\n",
    "    'THX': 'thanks',\n",
    "    'NP': 'no problem',\n",
    "    'WTF': 'what the fudge',\n",
    "    'ICYMI': 'in case you missed it',\n",
    "    'AFK': 'away from keyboard',\n",
    "    'BRB': 'be right back',\n",
    "    'DM': 'direct message',\n",
    "    'SMH': 'shaking my head',\n",
    "    'LMAO': 'laughing my ass off',\n",
    "    'GR8': 'great',\n",
    "    'OMW': 'on my way',\n",
    "    'ROFLMAO': 'rolling on the floor laughing my ass off',\n",
    "    'ICYMI': 'in case you missed it',\n",
    "    'TL;DR': 'too long; didn‚Äôt read',\n",
    "    'YOLO': 'you only live once',\n",
    "    'FOMO': 'fear of missing out',\n",
    "    'NSFW': 'not safe for work',\n",
    "    'OOTD': 'outfit of the day',\n",
    "    'IRL': 'in real life',\n",
    "    'AMA': 'ask me anything',\n",
    "    'SMH': 'shaking my head',\n",
    "    'Bae': 'before anyone else',\n",
    "    'DM': 'direct message',\n",
    "    'FOMO': 'fear of missing out',\n",
    "    'IMO': 'in my opinion',\n",
    "    'RN': 'right now',\n",
    "    'TBH': 'to be honest',\n",
    "    'ICYMI': 'in case you missed it',\n",
    "    'NVM': 'never mind',\n",
    "    'FWIW': 'for what its worth',\n",
    "    'BTW': 'by the way',\n",
    "    'POV': 'point of view',\n",
    "    'OMG': 'oh my god',\n",
    "    'LOL': 'laugh out loud',\n",
    "    'BRB': 'be right back',\n",
    "    'IDK': \"I don't know\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed0cef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0202d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "858cd08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in my humble opinion he is the best be right back'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('IMHO he is the best BRB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a1edc",
   "metadata": {},
   "source": [
    "# Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa96d147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f52168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa8644",
   "metadata": {},
   "source": [
    "***This line of code imports the TextBlob class from the textblob library. The TextBlob class is the main entry point for working with textual data using the TextBlob library. It provides various methods for text analysis, such as sentiment analysis, part-of-speech tagging, noun phrase extraction, and more.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a687aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions during several generations are modified in the same manner.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
    "\n",
    "textBlb = TextBlob(incorrect_text)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83863d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sample text for analysis.\"\n",
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69a3b7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "sentiment = blob.sentiment\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8110b09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'His is a sample text for analysis.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fea3e",
   "metadata": {},
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af32a886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c309cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14b63fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed3aebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79933aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably  all-time favorite movie,  story  selflessness, sacrifice  dedication   noble cause,    preachy  boring.   never gets old, despite   seen   15   times'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8a405fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This   example text containing  words   common stopwords. Stopwords  often removed  improve text analysis.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"This is an example text containing some words that are common stopwords. Stopwords are often removed to improve text analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5495cf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7265c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedee68a",
   "metadata": {},
   "source": [
    "# Remove Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f98a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20900cf3",
   "metadata": {},
   "source": [
    " ***import re: This line imports the re module, which provides support for regular expressions in Python.***\n",
    "\n",
    "***def remove_emoji(text):: This defines a function named remove_emoji that takes a single argument text, which is the input text from which you want to remove emojis.***\n",
    "\n",
    "***emoji_pattern = re.compile(\"[ ... ]+\", flags=re.UNICODE): This line defines a regular expression pattern to match emojis. The pattern is a combination of Unicode code point ranges for different categories of emojis. Unicode code points are numerical representations of characters, including emojis.***\n",
    "\n",
    "***The code includes multiple ranges of Unicode code points corresponding to different categories of emojis. For example, u\"\\U0001F600-\\U0001F64F\" represents a range of emoticon emojis. Other ranges are specified for symbols, transport symbols, flags, and more.***\n",
    "\n",
    "***flags=re.UNICODE: This flag ensures that the regular expression operates in Unicode mode, allowing it to correctly handle emojis represented by Unicode characters.***\n",
    "\n",
    "***return emoji_pattern.sub(r'', text): This line uses the sub() method of the emoji_pattern regular expression to substitute matched emojis with an empty string (r''). This effectively removes the emojis from the input text.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55362940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the Nature. It was '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Loved the Nature. It was üòòüòò\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94e1ae20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lmao '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Lmao üòÇüòÇ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a7d36f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
      "                                              0.0/358.9 kB ? eta -:--:--\n",
      "     ---                                      30.7/358.9 kB ? eta -:--:--\n",
      "     -----------                            112.6/358.9 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------------                   194.6/358.9 kB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------                  204.8/358.9 kB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------                  204.8/358.9 kB 1.4 MB/s eta 0:00:01\n",
      "     -----------------------------          276.5/358.9 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  358.4/358.9 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 358.9/358.9 kB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab59440",
   "metadata": {},
   "source": [
    "## In this example, the emoji library is used to convert emojis to text representations, text representations to emojis, and extract emojis from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ea86c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :fire:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('Python is üî•'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1184d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was  :smiling_face_with_smiling_eyes:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('Loved the movie. It was üòä'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1c6c88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :fire: and fun! :grinning_face_with_smiling_eyes:\n"
     ]
    }
   ],
   "source": [
    "text_with_emojis = \"Python is üî• and fun! üòÑ\"\n",
    "converted_text = emoji.demojize(text_with_emojis)\n",
    "print(converted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26006182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I :heart: Python üöÄ\n"
     ]
    }
   ],
   "source": [
    "text_with_aliases = \"I :heart: Python :rocket:\"\n",
    "emoji_text = emoji.emojize(text_with_aliases)\n",
    "print(emoji_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb213c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f27efe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['üöÄ']\n"
     ]
    }
   ],
   "source": [
    "def extract_emojis(text):\n",
    "    emoji_pattern = re.compile(r':[^:]+:')\n",
    "    emoji_aliases = emoji_pattern.findall(text)\n",
    "    emojis = [emoji.emojize(alias) for alias in emoji_aliases]\n",
    "    return emojis\n",
    "\n",
    "text_to_extract = \"üëã Hello, üåç world! :rocket:\"\n",
    "extracted_emojis = extract_emojis(text_to_extract)\n",
    "print(extracted_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f9336",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "**Tokenization is the process of breaking a text or a sequence of characters into smaller units called tokens. Tokens are the fundamental building blocks of text analysis and natural language processing (NLP). In English, tokens are typically words, but they can also be sentences, phrases, or even individual characters, depending on the context and task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7074204",
   "metadata": {},
   "source": [
    "## Key Points:\n",
    "\n",
    "**1.Word Tokenization: In word tokenization, a text is divided into individual words. For example, the sentence \"I love programming\" would be tokenized into the tokens \"I,\" \"love,\" and \"programming.\"**\n",
    "\n",
    "**2.Sentence Tokenization: In sentence tokenization, a text is divided into sentences. For example, the paragraph \"NLTK is great. It helps with NLP tasks.\" would be tokenized into the sentences \"NLTK is great.\" and \"It helps with NLP tasks.\"**\n",
    "\n",
    "**3.Importance: Tokenization is a crucial preprocessing step in NLP. It provides the foundation for various tasks, including sentiment analysis, language modeling, machine translation, and more.**\n",
    "\n",
    "**4.Whitespace and Punctuation: Tokenization is often based on whitespace (spaces, tabs, line breaks) and punctuation marks. Punctuation can be retained as separate tokens or combined with adjacent words, depending on the application.**\n",
    "\n",
    "**5.Ambiguities: Tokenization can be challenging for languages with complex word structures, such as agglutinative languages (e.g., Turkish) and languages with no clear word boundaries (e.g., Chinese).**\n",
    "\n",
    "**6.Subword Tokenization: Subword tokenization breaks down words into smaller subword units, which can be useful for handling rare or out-of-vocabulary words and for applying techniques like Byte-Pair Encoding (BPE).**\n",
    "\n",
    "**7.NLTK and Other Libraries: Libraries like NLTK, SpaCy, and the tokenize module in Python provide tools for tokenization. Different libraries might use different strategies and algorithms.**\n",
    "\n",
    "**Example:\n",
    "Consider the sentence: \"NLTK is a library for natural language processing.\" Word tokenization would result in the following tokens: \"NLTK,\" \"is,\" \"a,\" \"library,\" \"for,\" \"natural,\" \"language,\" \"processing.\"**\n",
    "\n",
    "**And for sentence tokenization, the text \"NLTK is great. It helps with NLP tasks.\" would be tokenized into the sentences: \"NLTK is great.\" and \"It helps with NLP tasks.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd74346",
   "metadata": {},
   "source": [
    "## 1. Using the split function¬∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b968d509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'dhaka']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "sent1 = 'I am going to dhaka'\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "61e466c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to Dhaka',\n",
       " ' I will stay there for 3 days',\n",
       " \" Let's hope the trip to be great\"]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sent2 = 'I am going to Dhaka. I will stay there for 3 days. Let\\'s hope the trip to be great'\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "84a41fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'dhaka!']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problems with split function\n",
    "sent3 = 'I am going to dhaka!'\n",
    "sent3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fb86d794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think I should go? I have 3 day holiday']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 = 'Where do think I should go? I have 3 day holiday'\n",
    "sent4.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf04ea9",
   "metadata": {},
   "source": [
    "## 2. Regular Expression\n",
    "**A regular expression (regex or regexp) is a sequence of characters that defines a search pattern. It is a powerful tool used for pattern matching and manipulation of strings. Regular expressions are widely used in programming, text processing, and various applications, including data validation, searching, and text manipulation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c69201",
   "metadata": {},
   "source": [
    "## key Concepts:\n",
    "\n",
    "1. Pattern Matching: Regular expressions allow you to specify a pattern that defines the structure and content of text you want to match.\n",
    "\n",
    "2. Metacharacters: Regular expressions use special characters, known as metacharacters, to represent classes of characters, repetitions, alternatives, and more.\n",
    "\n",
    "3. Quantifiers: Quantifiers indicate the number of occurrences of a character or group. Common quantifiers include * (zero or more), + (one or more), ? (zero or one), {n} (exactly n occurrences), {n,} (n or more occurrences), and {n,m} (between n and m occurrences).\n",
    "\n",
    "4. Character Classes: Character classes define sets of characters that can match at a particular position. For example, [a-z] matches any lowercase letter, and \\d matches a digit.\n",
    "\n",
    "5. Anchors: Anchors specify positions in the string where a match should occur. Common anchors include ^ (start of line), $ (end of line), and \\b (word boundary).\n",
    "\n",
    "6. Groups and Alternation: Parentheses () are used to create groups, which can be used for capturing or grouping subpatterns. The vertical bar | represents alternation, allowing you to match one of multiple alternatives.\n",
    "\n",
    "7. Escape Sequences: Some characters have special meanings in regular expressions. To match these characters literally, you need to escape them using a backslash \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf307f",
   "metadata": {},
   "source": [
    "## Example:\n",
    "\n",
    "### Consider the regular expression: ^[\\w.-]+@\\w+\\.\\w+$\n",
    "\n",
    "This regex matches email addresses with the following structure\n",
    "\n",
    "1. ^: Start of the line.\n",
    "2. [\\w.-]+: One or more word characters, dots, or hyphens (matches the username part of the email).\n",
    "3. @: Matches the at symbol.\n",
    "4. \\w+: One or more word characters (matches the domain name).\n",
    "5. \\.: Matches a dot.\n",
    "6. \\w+: One or more word characters (matches the top-level domain, like \".com\").\n",
    "7. $: End of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bf598bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'dhaka']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent3 = 'I am going to dhaka!'\n",
    "tokens = re.findall(\"[\\w']+\", sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "714ea8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry',\n",
       " \"\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789ac94",
   "metadata": {},
   "source": [
    "## 3. NLTK\n",
    "\n",
    "**NLTK (Natural Language Toolkit) is a powerful Python library that provides tools and resources for working with human language data, particularly in the field of natural language processing (NLP). It offers various functionalities for tasks such as tokenization, stemming, lemmatization, part-of-speech tagging, parsing, and more.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2fe61",
   "metadata": {},
   "source": [
    "## Key Features and Components:\n",
    "1. Tokenization: NLTK provides methods for tokenizing text into sentences and words. The sent_tokenize() and word_tokenize() functions are commonly used for this purpose.\n",
    "2. Stemming and Lemmatization:  \n",
    "    * NLTK includes stemmers and lemmatizers for reducing words to their root forms. Common stemmers include the Porter Stemmer and Snowball Stemmer.\n",
    "    * The WordNet Lemmatizer is available for lemmatization, which reduces words to their base or dictionary forms.\n",
    "\n",
    "3. Part-of-Speech Tagging:\n",
    "     * NLTK offers part-of-speech tagging, which identifies the grammatical parts of words in a sentence.\n",
    "   * The pos_tag() function assigns parts of speech to each word in a sentence.\n",
    "\n",
    "4. Corpora and Datasets:\n",
    "\n",
    "   * NLTK provides a variety of pre-processed corpora and datasets for various languages and domains. These corpora are useful for research, experimentation, and training models.\n",
    "   \n",
    "5. Text Classification:\n",
    "\n",
    "   * NLTK supports text classification tasks, such as sentiment analysis, using various machine learning algorithms.\n",
    "   \n",
    "6. Parsing and Chunking:\n",
    "\n",
    "   * NLTK includes parsers and chunkers that can analyze sentence structure and identify grammatical chunks.\n",
    "   \n",
    "7. Named Entity Recognition (NER):\n",
    "\n",
    "    * NLTK has tools for identifying named entities (e.g., names of people, organizations, locations) in text.\n",
    "    \n",
    "8. Frequency Distribution:\n",
    "\n",
    "   * NLTK's FreqDist class can compute frequency distributions of words in a text.\n",
    "   \n",
    "9. Concordance and Collocations:\n",
    "\n",
    "   * NLTK provides tools to find concordances (contexts where a word appears) and collocations (common word pairs) in a text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81bec1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "31af938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "018506c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'dhaka', '!']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = 'I am going to visit dhaka!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "43613ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "word_tokenize(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1de7eaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85406a7c",
   "metadata": {},
   "source": [
    "## 4. Spacy\n",
    "**SpaCy is an open-source natural language processing (NLP) library for Python that provides efficient and fast tools for working with textual data. It's designed to be both user-friendly and production-ready, making it a popular choice for various NLP tasks.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf693c0",
   "metadata": {},
   "source": [
    "## Key Features and Components:\n",
    "\n",
    "1. Tokenization: SpaCy performs efficient tokenization of text into words and sentences, including handling complex cases like contractions and punctuation.\n",
    "\n",
    "2. Part-of-Speech Tagging: SpaCy provides part-of-speech tagging to identify the grammatical parts of words in a sentence.\n",
    "\n",
    "3. Lemmatization: Lemmatization reduces words to their base or dictionary forms, helping to handle different forms of the same word.\n",
    "\n",
    "4. Dependency Parsing: SpaCy performs dependency parsing to determine the grammatical relationships between words in a sentence.\n",
    "\n",
    "5. Named Entity Recognition (NER): SpaCy identifies named entities such as names of people, organizations, and locations.\n",
    "\n",
    "6. Text Classification: SpaCy supports text classification tasks by training models on labeled data.\n",
    "\n",
    "7. Word Vectors: SpaCy provides pre-trained word vectors, such as Word2Vec and GloVe, which can be used for various NLP tasks.\n",
    "\n",
    "8. Customization: SpaCy allows you to train custom models on your own data for specific tasks.\n",
    "\n",
    "9. Language Support: While English is the most well-supported language, SpaCy also offers support for other languages.\n",
    "\n",
    "10. Performance: SpaCy is designed for efficiency and speed, making it suitable for both research and production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "757ef58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "12fbe74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a7a1ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "34c5d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n",
    "doc4 = nlp(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "922b3a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "going\n",
      "to\n",
      "visit\n",
      "dhaka\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc4:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ecfca63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We PRON\n",
      "'re AUX\n",
      "here ADV\n",
      "to PART\n",
      "help VERB\n",
      "! PUNCT\n",
      "mail VERB\n",
      "us PRON\n",
      "at ADP\n",
      "nks@gmail.com PROPN\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d5e7c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"SpaCy is a powerful NLP library.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4c2b56c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy PROPN\n",
      "is AUX\n",
      "a DET\n",
      "powerful ADJ\n",
      "NLP PROPN\n",
      "library NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513ac29",
   "metadata": {},
   "source": [
    "# Stemming\n",
    " \n",
    "**Stemming is a text normalization technique used in natural language processing (NLP) and information retrieval to reduce words to their base or root form. The idea behind stemming is to strip affixes (prefixes and suffixes) from words so that variations of a word can be treated as the same word.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffeb5f4",
   "metadata": {},
   "source": [
    "## Key Points:\n",
    "\n",
    "1. Purpose: Stemming aims to simplify words to their basic form, which can help improve text analysis, information retrieval, and various NLP tasks.\n",
    "\n",
    "**Example: For example, the words \"running,\" \"runner,\" and \"runs\" would all be stemmed to \"run.\"**\n",
    "\n",
    "2. Reduced Vocabulary: Stemming reduces the vocabulary size by considering different forms of the same word as a single word. This can help in tasks like text classification, where features are based on individual words.\n",
    "\n",
    "3. Algorithmic Approaches: Stemming algorithms use linguistic rules and heuristics to identify and remove prefixes and suffixes from words. Some common stemming algorithms include the Porter Stemmer and Snowball Stemmer (also known as the Porter2 Stemmer).\n",
    "\n",
    "4. Drawbacks: While stemming is a simple and effective way to normalize words, it can sometimes produce incorrect stems due to its heuristic nature. For example, \"better\" might be stemmed to \"better,\" while the correct root form is \"good.\"\n",
    "\n",
    "5. Usage: Stemming is often used when the specific linguistic meaning of words is less important than capturing their basic form. It's commonly used in search engines, information retrieval systems, and text preprocessing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "70b79143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f4f97956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'runner', 'run']\n"
     ]
    }
   ],
   "source": [
    "words = [\"running\", \"runner\", \"runs\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b29ad",
   "metadata": {},
   "source": [
    "**In this example, the words \"running,\" \"runner,\" and \"runs\" are stemmed to \"run,\" \"runner,\" and \"run\" respectively.**\n",
    "\n",
    "**Stemming can be a useful technique for basic text analysis tasks where capturing the general meaning of words is more important than preserving linguistic accuracy. However, for more advanced NLP tasks, lemmatization (which produces valid words) might be preferred over stemming.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c1439401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1238455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6d784ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "120773f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\n"
     ]
    }
   ],
   "source": [
    "text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5d4748ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "750f5b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ac1d8f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "856401d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d83f234",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "**Lemmatization is a natural language processing technique used to reduce words to their base or dictionary form, known as a lemma. The goal of lemmatization is to transform different inflected or derived forms of a word into a common base form, so that they can be analyzed or compared more effectively. Lemmatization is particularly useful for text analysis tasks like information retrieval, text classification, and language modeling.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fea79",
   "metadata": {},
   "source": [
    "### Here's an example to illustrate lemmatization:\n",
    "\n",
    "**Original words: \"running\", \"runner\", \"runs\"**\n",
    "\n",
    "**After lemmatization: \"run\"**\n",
    "\n",
    "**In this example, the lemmatizer identifies that \"running\", \"runner\", and \"runs\" are all forms of the same base word \"run\" and transforms them accordingly.**\n",
    "\n",
    "**Lemmatization is different from stemming, another text normalization technique. While stemming aims to remove prefixes and suffixes to find the root form of a word, lemmatization considers the context and meaning of the word to determine its base form. As a result, lemmatization tends to produce more accurate and meaningful results compared to stemming.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c7d3117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'runner', 'run']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"runner\", \"runs\"]\n",
    "lemmatized_words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f901625",
   "metadata": {},
   "source": [
    "# Lemma\n",
    "\n",
    "**A lemma, in linguistics, refers to the base or canonical form of a word. It is the form of the word that is typically found in dictionaries and represents the word's core meaning. Lemmas are used in various natural language processing tasks to simplify the analysis of text by reducing inflected or derived forms of words to a common base.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7959f6b",
   "metadata": {},
   "source": [
    "**For example, the lemma of the word \"running\" is \"run,\" the lemma of \"better\" is \"good,\" and the lemma of \"went\" is \"go.\"**\n",
    "\n",
    "\n",
    "**Lemmatization is the process of finding the lemma of a word. It involves considering the word's context and its part of speech to determine the appropriate base form. Lemmatization is commonly used in tasks such as text analysis, information retrieval, and language modeling to ensure that words with the same meaning are treated as equivalent, regardless of their inflected forms.**\n",
    "\n",
    "**In many languages, lemmatization requires access to language-specific linguistic resources, such as dictionaries and morphological rules, to accurately determine the lemmas of words. Natural language processing libraries like NLTK and spaCy provide tools for performing lemmatization in various languages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0b5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
